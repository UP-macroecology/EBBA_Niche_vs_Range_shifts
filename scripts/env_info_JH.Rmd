---
title: "CHELSA"
author: "Simon Kapitza and Katrin Schifferle"
date: "15/07/2022"
output:
  md_document:
    toc: true
    df_print: paged
bibliography: references.bib
---
 File pathaways made relative to ".Data/" need to be checked
```{r include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=120), tidy=FALSE)
knitr::opts_knit$set(root.dir = "C:\\Users\\James Hunter\\Desktop\\Hunter_NichevsRangeShift_2022")

```

# CHELSA data 

## General information
CHELSA environmental layers [@karger_climatologies_2017] are global maps of climate-model generated surface temperature and precipitation estimates, statistically downscaled to a spatial resolution of 30 arc sec. The data are available at different temporal aggregations (daily, monthly, as long-term climatology) for the time period 1980-2013. 
Since this code accompanies the Breeding Bird Survey (BBS) study, we process the data in two alternative ways: i) we crop the global data at the original resolution to the extent of the conterminous United States and ii) we reproject the data to a coarser resolution of 40 km (because this matches the buffer radius of the BBS route buffers) and crop it to the extent of the conterminous United States.

## Downloading CHELSA

The CHELSA repository is available [here](https://envicloud.wsl.ch/#/) to download layers through the browser, but we can also use R to download data through a script.

In the following code chunk, we download the required global climate layers for each month of the years 1980-1983 (the "historic" time period and the preceding year, but this is variable, depending on what we ultimately decide) into a user-specified folder. The downloaded data are pretty big, so we only download the variables we need to create the bioclim climatologies (monthly tasmax, tasmin, pr) and the mean monthly temperature (tas) for the time period.

```{r message=FALSE, include=FALSE}
# Export code from this markdown into R doc for testing etc.
knitr::purl(file.path("./code", "env_info.Rmd"), output = file.path("./code", "env_info.Rmd"))
```

```{r eval=FALSE}
# function to download CHELSA V_2.1 climate layers:
# chelsa_get2(): 
# slightly modified version of Simon Kapitzas function which can be found here:
# https://github.com/kapitzas/WorldClimTiles/blob/master/R/chelsa_get.R
# inputs:
# target_path = path to folder where downloaded data should be saved
# years = vector containing the years from which monthly data should be downloaded (must be between 1972-2013)
# vars = character string containing the CHELSA variables to be downloaded, can be one or more of "pr", "tas", "tasmax", "tasmin".
# months = Vector containing the months to be downloaded (1-12).
# method = method used by download.file() to download files. Type ?download.file for possible values.
chelsa_get2 <- function(target_path, 
                        years, 
                        vars,
                        months, 
                        method = "libcurl"){
  
  timeout_old <- getOption('timeout')
  options(timeout=1000)
  
  for(year in years){
    for(var in vars){
      for(month in months){
        
        month <- sprintf("%02d", month)
        name <- paste(c("CHELSA", var, month, year, "V.2.1.tif"), collapse = "_")
        source_url <- file.path("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/monthly", var, name)
        destination <- file.path(target_path, name)
        
        if(!file.exists(destination)){
          out <- NULL
          out <- tryCatch(download.file(source_url, destination, method = method), 
                          error = function(e) {return(NA)})
          if(is.na(out)){
            next
          }
        } else {
          message(paste0(destination, " already downloaded, skipping to next"))
        }
      }
    }
  }
  options(timeout = timeout_old)
}
# variables to be downloaded:
vars <- c("pr", "tas", "tasmax", "tasmin")
# time period for which data should be downloaded:
months <- 1:12
years <- c(1984:1990)
# path to folder where downloaded data should be saved:
target_path <- file.path("//ibb-fs01.ibb.uni-potsdam.de//users$//hunter") # "." = placeholder for the current working directory
# create destination folder if it doesn't exist yet:
if(!dir.exists(target_path)){
  dir.create(target_path, recursive = TRUE)
}
# download data (might take a while)
# if this does not work for you (e.g. creates files without content, try specifying 
# a different method, see ?download.file for options; depends on your operating system.
chelsa_get2(target_path, years, vars, months, method = "curl") # "libcurl" works on linux cluster

### Re-downloading layers that were buggy for reprojection to try again:
vars <- c("pr")
# time period for which data should be downloaded:
months <- 10
years <- c(2013)
# path to folder where downloaded data should be saved:
target_path <- file.path(".Data//CHELSA_global_climate_dat_1980-1990+1999-2019/redownloaded_buggy_layers") # "." = placeholder for the current working directory
# create destination folder if it doesn't exist yet:
if(!dir.exists(target_path)){
  dir.create(target_path, recursive = TRUE)
}
# download data (might take a while)
# if this does not work for you (e.g. creates files without content, try specifying 
# a different method, see ?download.file for options; depends on your operating system.
chelsa_get2(target_path, years, vars, months, method = "curl")
```

## Reprojecting and masking CHELSA layers

Now we have the data in our target folder. Before processing them into bioclim variables for the selected time periods, let's crop them to the extent of European continent to reduce processing time. For this, it is useful to first create mask rasters. These rasters have the value 1 in all terrestrial areas and `NA` over water and outside the study area. They should have the same resolution, projection and extent as the final grids: for our two methods, we need resolutions of 1km and 40km. Thus, we first need to project both mask rasters using an equal area projection, so that afterwards raster cells all have the same size (this is not the case in a lat/long grid, because area gets distorted towards the poles).

To create the masks, we can use the EBBA2 shapefile which contains the extent of the new Atlas grid coverage. This is enclosed in the `input_data` folder of this repository. 

```{r eval=FALSE}

# if package is not installed yet, install it with install.packages()
require(geodata) # for gadm()
require(terra)
require(dplyr)
require(raster)
require(gdalUtilities) # package to use gdal functions via R (does not require installing GDAL)
EBBA2_Grid <- file.path(".", "input_data", "EBCC", "EBBA2", "ebba2_grid50x50_v1")

# read in rasterized EBBA2 grid

# project mask from lat/lon to equal area projection of the two final grids:
input <- file.path(".", "input_data", "Europe_ebba_grid_50km.tif")

#testing simpler code#
testmask<-rast(input)
plot(testmask)
#writeRaster(EuFishnet,file=file.path("C:/Users/James Hunter/Desktop/Hunter_NichevsRangeShift_2022", "input_data", "Europe_mask.asc"),overwrite=T)

#read in downloaded chelsa data
target_path <- file.path(".Data//CHELSA_global_climate_dat_1980-1990+1999-2019")

chelsa_data <- list.files(target_path, full.names = TRUE, pattern = "1.tif")

#Assign 2 test layers
testCHELSApr<-rast(chelsa_data[1])
testCHELSAtem<-rast(chelsa_data[500])

#transform mask to equal area proj (albers europe)
transformedmask<-terra::project(x=testmask,y="ESRI:102013",mask=T)

#checks
transformedmask
plot(transformedmask)

#retrieve extent
mask_ext <- ext(transformedmask) 

# projection (equal area projection for Europe):
albers_projection <- "ESRI:102013" # attention: "ESRI" not "EPSG"


```

With mask we can easily crop and reproject the downloaded CHELSA data to match with these grids. The following code chunk achieves that.

```{r eval=FALSE}
# folder to which raw CHELSA were downloaded above:
chelsa_names <- list.files(target_path, full.names = FALSE, pattern = "1.tif") # only file names without path
# path to folder for reprojected CHELSA data:
reproj_path <- file.path(".Data/", "Chelsa_ebba_reprojection_5km_ee")
# create folder if it doesn't exist yet:
if(!dir.exists(reproj_path)){
  dir.create(reproj_path, recursive = TRUE)
}
resolutions <- c("50km")

###Identify and investigate buggy chelsa layers!###
##58,89,151,387 bugged AGAIN! redownloading was not the issue ...
#Buggylayers<- chelsa_data[c(58,151,235,311)]
#Buggylayers

# loop through the two resolutions:
for(res in resolutions){
  
  print(paste0("Processing resolution ", res, "."))
  
  # read mask:
  mask_res <- transformedmask
  # extent of the mask:
  mask_ext <- ext(mask_res)
  
  # create list of file paths for the reprojected data:
  names <- paste0(unlist(lapply(chelsa_names, FUN = function(x) {strsplit(x, "tif")})), paste0(res, ".tif"))
  names <- file.path(reproj_path, names)
  
  # loop through downloaded CHELSA layers and reproject and mask them
  for(i in 1:length(chelsa_data)){
    
    print(paste(i, "of", length(chelsa_data)))
    
    # reproject chelsa data:
    gdalUtilities::gdalwarp(srcfile = chelsa_data[i],
                            dstfile = names[i],
                            overwrite = TRUE,
                            tr = res(mask_res),
                            r = "bilinear", # resampling method
                            
                            s_srs = "EPSG:4326",
                            t_srs = albers_projection,
                            te = c(mask_ext[1], mask_ext[3], mask_ext[2], mask_ext[4])
                            ) 
    
    # mask reprojected raster so all non-terrestrial areas become NA:
    names[i] %>% 
      rast %>% 
      mask(mask = mask_res) %>% 
      # overwrite reprojected raster with masked raster:
      writeRaster(names[i], overwrite = TRUE)
    }
}
```

We can now use these layers to calculate `bioclim` variables for the selected time period covering the conterminous US at 1km and 40km resolution.

``` {r eval = FALSE}
require(dismo) # requires spatial data formats of raster package -> no transition to terra possible
require(raster)
require(stringr)
# path to folder for bioclim variables:
bioclim_path <- file.path(".Data//Europe_Bioclimate_1981-90_2009-2018")
# create folder if it doesn't exist yet:
if(!dir.exists(bioclim_path)){
  dir.create(bioclim_path, recursive = TRUE)
}
# variables, months and the two resolutions for the output
vars <- c("pr_", "tas", "tasmin_", "tasmax_")
months <- str_pad(1:12, width = 2, pad = "0")
resolutions <- c("50km")
# loop through resolutions
for(res in resolutions){
  
  print(paste0("Processing resolution ", res, "."))
  
  # list monthly CHELSA data for europe 2009-2018 (repeeated with 1981-1990 data too)
  chelsa <- list.files(file.path(".Data//Chelsa_ebba_reprojection_5km_ee/1980-1990"), pattern = res, full.names = TRUE)
  years<-1981:1990
  
  # load mask (use raster package since this is used to create a spatial brick which is required by dismo::biovars())
  mask <- raster(chelsa[1])
  # pull out non-NA cells to reduce processing time
  mask_inds <- which(!is.na(mask[]))
  
  # create a spatial brick template from the mask and 
  # create bricks to store the month-wise means across the selected years for each bioclim variable
  out <- brick(mask, values = FALSE)
  pr_mean <- tasmin_mean <- tasmax_mean <- out
  
  # loop through months
  for(j in as.numeric(months)){
    
    print(paste("month", j))
    
    # calculate precipitation mean for the current month (across all available years)
    pr_mean[[j]] <- chelsa[which(grepl(paste0("pr_", months[j]), chelsa))] %>% 
      stack %>% 
      mean
    
    # calculate tasmin mean for the current month (across all available years)
    tasmin_mean[[j]] <- chelsa[which(grepl(paste0("tasmin_", months[j]), chelsa))] %>% 
      stack %>% 
      mean
    
    # calculate tasmax mean for the current month (across all available years)
    tasmax_mean[[j]] <- chelsa[which(grepl(paste0("tasmax_", months[j]), chelsa))] %>% 
      stack %>% 
      mean
  }
  
  # pull out only the non-NA values (terrestrial area, makes the biovars function twice as fast)
  #pr_mean <- pr_mean[mask_inds]
  #tasmin_mean <- tasmin_mean[mask_inds]
  #tasmax_mean <- tasmax_mean[mask_inds]
  
  # calculate bioclim variables (be very patient!)
  biovars <- dismo::biovars(prec = pr_mean,  
                            tmin = tasmin_mean, 
                            tmax = tasmax_mean
  )
  
  # create empty spatial brick for output and write bioclim data in it:
 # out@data@nlayers <- as.integer(19)
  
 # for(k in 1:nlayers(out)){
#    out[[k]][mask_inds] <- biovars[,k]
#    print(k)
#  }
  
#  names(out) <- colnames(biovars)
  
  # write individual rasters into files:
  raster::writeRaster(biovars, 
                      bylayer = TRUE, 
                      filename = file.path(bioclim_path, paste0("CHELSA_", names(biovars), "_", 
                                                                min(years), "_", 
                                                                max(years), "_", 
                                                                res, ".tif")), 
                      overwrite = TRUE)
}
```


# LUH2 data
## General information
The Land-Use Harmonization 2 (LUH2) project under which the LUH2 land use data were produced aim to harmonize historical reconstructions of land use with future projections in a format required for Earth System Models (EMS), which are used in scenario exploration under CMIP6 [@hurtt_harmonization_2020]. The data is available globally at a spatial resolution of 0.25 degrees (27-28km around the equator, but shrinking toward the poles). The historical reconstruction comprises layers from 850-2015, future predictions are made under the assumptions of different Shared Socio-economic Pathways (SSP) [@popp_land-use_2017].

These are the LUH2 land use classes:
```{r eval=FALSE}
# primf: forested primary land
# primn: non-forested primary land
# secdf: potentially forested secondary land
# secdn: potentially non-forested secondary land
# pastr: managed pasture
# range: rangeland
# urban: urban land
# c3ann: C3 annual crops
# c3per: C3 perennial crops
# c4ann: C4 annual crops
# c4per: C4 perennial crops
# c3nfx: C3 nitrogen-fixing crops
# secma: secondary mean age (units: years)
# secmb: secondary mean biomass density (units: kg C/m^2)
```

## Downloading LU2
Different releases of the data are available [here](https://luh.umd.edu/data.shtml). We have already downloaded the LUH2 v2h release (10/14/16, historical reconstruction 850-2015) to our group's data share. The data set is provided in `*.nc` (netCDF) format.

With the data on the data share, we can now crop and project the data to match the BBS study. Note that LUH2 data are not available at the fine 1km resolution, so instead of buffering from 1km cells (method i)) we only project to an equal area representation, keeping the resolution approximately the same for this method (around 27km at the equator).

```{r eval=FALSE}
require(ncdf4)
require(gdalUtilities)
# Use reprojected CHELSA data as the mask:
input <- file.path(".Data//Chelsa_ebba_reprojection_5km_ee/1980-1990/CHELSA_pr_01_1981_V.2.1.50km.tif")
# projection (equal area projection for North America):
albers_projection <- "ESRI:102013" # attention: "ESRI" not "EPSG"
# get extent of projected mask:
mask_ext <- input %>% 
  rast %>% 
  ext() # get extent

# use masks to extract LUH2 raw data:
# read netcdf file with raw data:
luh2_path <- "//ibb-fs01.ibb.uni-potsdam.de/daten$/AG26/Arbeit/datashare/data/envidat/socioeconomic/LUH2_v2h/global" # from cluster: "/mnt/ibb_share/zurell/envidat/socioeconomic/LUH2_v2h/global"  
luh2 <- ncdf4::nc_open(file.path(luh2_path, "states.nc"))
# names of the land-use classes:
lu_classes <- names(luh2$var)[1:14]
# years for which data should be extracted-No data beyond 2015 in download:
years <- c(1980:2015)
# resolutions for which data should be extracted:
resolutions <- c("50km")
# path to folder for extracted LUH2 data:
luh2_data_path <- file.path(".Data//LUH2_DAT")
# create folder if it doesn't exist yet:
if(!dir.exists(luh2_data_path)){
  dir.create(luh2_data_path, recursive = TRUE)
}
# loop over land use classes:
for(lu in lu_classes){
  
  print(paste("LU class", which(lu_classes == lu), "of", length(lu_classes)))
  
  # load all LUH2 data from current class (= each year from 850-2015, for whole globe)
  luh2_lu <- rast(file.path(luh2_path, "states.nc"), subds = lu)
  names(luh2_lu) <- as.character(850:2015)
  
  # filter only the years we are interested in:
  luh2_lu_out <- luh2_lu[[as.character(years)]] 
  
  # write the layers to file so we can use them with gdalUtilities:
  writeRaster(luh2_lu_out, file.path(luh2_data_path , paste0(lu, "_", names(luh2_lu_out), "_luh2.tif")), overwrite = TRUE)
  
  # reproject to 50km resolution and crop to mask:
  
  # files of current land use class:
  files <- list.files(file.path(luh2_data_path), full.names = TRUE, pattern = paste0(lu, ".*luh2.tif$")) # name contains land use class and "luh2.tif" but not the .aux extention ($)
  # loop over resolutions:
  for(res in resolutions){
    # mask for current resolution
    mask <- rast( file.path(".Data//Chelsa_ebba_reprojection_5km_ee/1980-1990/CHELSA_pr_01_1981_V.2.1.50km.tif")
)
    # mask extent:
    ext_mask <- ext(mask)
    # output file names
    names <- paste0(unlist(lapply(files, FUN = function(x) {strsplit(x, "tif")})), paste0(res, ".tif"))
  
    # loop over years:
    for(i in 1:length(files)){
      
      input <- files[i]
      output <- names[i]
      
      # reproject to 27 km resolution:
      gdalUtilities::gdalwarp(srcfile = files[i],   
                              dstfile = names[i], 
                              overwrite = TRUE,
                              tr = res(mask), # target resolution
                              r = "bilinear", # resampling method
                              t_srs = albers_projection, # target crs
                              te = c(ext_mask[1],ext_mask[3],ext_mask[2],ext_mask[4]), # extent of output file to be created
                              dryrun = FALSE)
  
    # crop to mask:
    names[i] %>% 
      rast() %>% 
      mask(mask = mask) %>% 
      # overwrite reprojected raster with reprojected and masked raster:
      writeRaster(names[i], overwrite = TRUE)
      }
  }
  # delete raw rasters with global data:
  unlink(files)
  # delete aux.json files:
  unlink(list.files(file.path(luh2_data_path), full.names = TRUE, pattern = "aux.json"))
}
```

